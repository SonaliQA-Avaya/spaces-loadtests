# Test Spaces2 setup in project "spaces-2018"

## Test socket

Test how many connections one pod can handle

### Test 1

Test with artillery, command: `node run.js -- connection.yaml`

Input:
- `spaces-us` cluster size: 10 * 2 (zones) ns-standard-2
- deployment `spaces-socket` size: 1, readiness check is /ready, no resources limit
- deploymet `spaces-socket-hpa` is removed
- `spaces-socket` is the only pod running in the node (except for fluentd and kube-proxy)
- MongoDB is using mlab `AvayaStore1`, version 3.4.10 (WiredTiger), GCP us-central1 plan M2 Standard (4G memory, 60G SSD), with preferSSL
- Cluster loadtest-artillery instances, pods: 6, 6
- config.socketio.phases.0.duration: 1200
- config.socketio.phases.0.arrivalRate: 7
- scenarios.0.flow.1.think: 1200
- ==> max connections can generate with 6 workers: 7 * 1200 * 6 = 50,400

Result:
- Max socketio connection: 11610
- Max pod CPU: 134%
- Max pod memory: 1.75G
- Max node CPU: 140%
- Max node memory: 1.87G 
- Connection started to drop after 11K, pod turned into unready

### Test 2

Test with `/testClient/socketio_benchmark/test_max_connection.js`, command: `DEBUG=socketiotester node test_max_connection.js`.

Input:
- `spaces-us` cluster size: 10 * 2 (zones) ns-standard-2
- deployment `spaces-socket` size: 1, readiness check is /ready, no resources limit
- deploymet `spaces-socket-hpa` is removed
- `spaces-socket` is the only pod running in the node (except for fluentd and kube-proxy)
- MongoDB is using mlab `AvayaStore1`, version 3.4.10 (WiredTiger), GCP us-central1 plan M2 Standard (4G memory, 60G SSD), with preferSSL
- Load generator, `loadtest-socketio` VM of n1-standard-16 (16 vCPUs, 60 GB memory)
- clientCount: 2000
- connection_batch_size: 2000
- start test on each SSH connection
- ==> max connections can generate with 8 workers: 8 * 2000 = 16,000

Result:
- Max socketio connection: 15007
- Max pod CPU: 117%
- Max pod memory: 1.67G
- Max node CPU: 122%
- Max node memory: 1.8G
- Connection started to drop/reconnect after 15K, pod turned into unready and recovered
- After connection droped to 14K connection, pod became stable
