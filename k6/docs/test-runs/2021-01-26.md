target system: https://spaces-standalone.esna.com/


## using feature/candidate before DS changes

version: gcr.io/onesnastaging/spaces:v3.6.1-f21fd1f-25d9371


### 9,400 vus
no issues (in Sep failed at 45k VUS, 3100 join/s)

join rate: 5k (6k in GCP dash)
RPS: 2160 

frontend: 379 pods
socket: 48 pods

mongo: CPU up to 70%

Redis:
  socket:  50,000 - Ops/s, used memory 60Mb
  cache: 3,200 - ops/s, used memory 40Mb


## 10,000 vus 

join rate: 6650/m
backend latency: bellow 600ms - **GOOD**
RPS: 3100

pods:
  frontend: 438 
  socket: 50


errors:
  frontend: 500
  socket: 900

second attempt: container errors, up to 2000/m


## 12,000 vus 

started to have issues around 6-7k vus, not clear if the system is considered stable or not 


## 12,000 vus FC with DS 

version: gcr.io/onesnastaging/spaces:v3.6.1-221cc49-636398a-12


actual vus: 11,8k

join rate: 7100 
latency: p95~1s
RPS: 4000

frontend: 444 pods
socket: 57
task: 1
taskpoll: 2

[google dashboard](https://console.cloud.google.com/monitoring/dashboards/builder/63b859e2-691d-4bb4-b345-f9d2888d1f1a?project=prodigy-dev-1&dashboardBuilderState=%257B%2522editModeEnabled%2522:false%257D&startTime=20210128T170029-05:00&endTime=20210128T194000-05:00)

  - backend latency is messed up because it includes `api/taskqueue/runner` calls, which are slower than normal calls
  solved by disabling log for task backend

[DD dashboard](https://app.datadoghq.com/dashboard/r6i-hui-6wa?from_ts=1611869755038&live=false&to_ts=1611880842114)


### 12,000 vus FC with DS - with fixed `markread` calls 

at 5100 join rate spike in socket errors

~8k vus  (9100 vus)
2570 RPS


### 9,000 vus 

Looks fine overall, except for latency 

join rate: 6100/m
backend latency: p95=1.25s
container errors: up to 800/m - 0.5% - OK 


### 10,000 vus

join rate up to 6500/m 
RPS: 2600
backend latency: p95=1.4s
container errors - up to 2000/m, at 150k requests, which is ~ 1.3% - **FAILED**




## multi-cluster-2 (k8s prod manifests)

version: gcr.io/onesnastaging/spaces:v3.6.1-948f724-05da5bc-12 


### 10,000 vus

with prod base more pods are created in socket, it never grew above 70 socket pods with staging k8s manifests, and it's already > 200 pods at 3000 join rate


there were less errors in socket (200 errors/m max), but backend latency p95 went up to 9s **FAIL**


# Findings 

Data Sovereignty introduces increased latency (or is it node 12?)

Production manifests create more socket pods which improves socket errors problem, but frontend pods are not scaled properly API 
